/* model.add(tf.layers.lstm({ units: 64, activation: 'relu', inputShape: [standardized_features[0].length, standardized_features[0][0].length], returnSequences: true }))
model.add(tf.layers.lstm({ units: 32, activation: 'relu', returnSequences: false }))
model.add(tf.layers.dropout({ rate: 0.2 }))
model.add(tf.layers.dense({ units: standardized_labels[0].length })) */

// Normalize data (min-max scaling)
function normalizeData(data) {
    const min = tf.min(tf.tensor(data), 0);
    const max = tf.max(tf.tensor(data), 0);
    const normalized = tf.div(tf.sub(tf.tensor(data), min), tf.sub(max, min));
    return normalized.arraySync();
}

function normalizeTensorFit(tensor) {
    const maxval = tensor.max();
    const minval = tensor.min();
    const normalizedTensor = normalizeTensor(tensor, maxval, minval);
    return [normalizedTensor, maxval, minval];
}

function normalizeTensor(tensor, maxval, minval) {
    const normalizedTensor = tensor.sub(minval).div(maxval.sub(minval));
    return normalizedTensor;
}

/* const n_layers = 4;
    const learning_rate = 0.01;

    // input dense layer
    const input_layer_shape = timeStep;
    const input_layer_neurons = 64;

    // LSTM
    const rnn_input_layer_features = 16;
    const rnn_input_layer_timesteps = Math.floor(input_layer_neurons / rnn_input_layer_features);
    const rnn_input_shape = [rnn_input_layer_features, rnn_input_layer_timesteps]; // the shape have to match input layer's shape
    const rnn_output_neurons = 16; // number of neurons per LSTM's cell

    // output dense layer
    const output_layer_shape = rnn_output_neurons; // dense layer input size is same as LSTM cell
    const output_layer_neurons = 1; // return 1 value

    console.log(rnn_input_layer_features, rnn_input_layer_timesteps, rnn_input_shape)

    model.add(tf.layers.dense({ units: input_layer_neurons, inputShape: [input_layer_shape] }));
    model.add(tf.layers.reshape({ targetShape: rnn_input_shape }));

    let lstm_cells = [];
    for (let index = 0; index < n_layers; index++) {
        lstm_cells.push(tf.layers.lstmCell({ units: rnn_output_neurons }));
    }

    model.add(tf.layers.rnn({
        cell: lstm_cells,
        inputShape: rnn_input_shape,
        returnSequences: false
    }));

    model.add(tf.layers.dense({ units: output_layer_neurons, inputShape: [output_layer_shape] })); */

/* model.add(tf.layers.lstm({ units: 64, activation: 'relu', inputShape: [xTrain.shape[1], xTrain.shape[2]], returnSequences: true }))
    model.add(tf.layers.lstm({ units: 32, activation: 'relu', returnSequences: false }))
    model.add(tf.layers.dropout({ rate: 0.2 }))
    model.add(tf.layers.dense({ units: y_Train.length })) */

/* log.info('----> Step 5 : Normalizing the data')
    // Define labels (e.g., predict the close price at the next time step)
    const labels = ohlcvData.map(item => item[3]);
    console.log('Labels : ', labels.length, labels[0])

    const inputTensor = tf.tensor2d(features, [features.length, features[0].length])
    const labelTensor = tf.tensor2d(labels, [labels.length, 1])

    console.log('Input Tensor : ', inputTensor.shape)
    console.log('Label Tensor : ', labelTensor.shape)

    const [xs, inputMax, inputMin] = normalizeTensorFit(inputTensor)
    const [ys, labelMax, labelMin] = normalizeTensorFit(labelTensor)

    console.log('Normalized Input Tensor : ', xs.shape)
    console.log('Normalized Label Tensor : ', ys.shape) */



// Normalize the features (important for neural networks)
// const normalizedFeatures = normalizeData(features);
// console.log('Normalized Features Length : ', normalizedFeatures[0])

/* const n_layers = 4;
    const learning_rate = 0.01;

    // input dense layer
    const input_layer_shape = features[0].length;
    const input_layer_neurons = 64;

    // LSTM
    const rnn_input_layer_features = 16;
    const rnn_input_layer_timesteps = Math.floor(input_layer_neurons / rnn_input_layer_features);
    const rnn_input_shape = [rnn_input_layer_features, rnn_input_layer_timesteps]; // the shape have to match input layer's shape
    const rnn_output_neurons = 16; // number of neurons per LSTM's cell

    console.log(rnn_input_layer_features, rnn_input_layer_timesteps, rnn_input_shape) */

// output dense layer
/* const output_layer_shape = rnn_output_neurons; // dense layer input size is same as LSTM cell
const output_layer_neurons = 1; // return 1 value */

/* const model = tf.sequential();
    model.add(tf.layers.dense({ units: input_layer_neurons, inputShape: [input_layer_shape] }));
    model.add(tf.layers.reshape({ targetShape: rnn_input_shape }));

    let lstm_cells = [];
    for (let index = 0; index < n_layers; index++) {
        lstm_cells.push(tf.layers.lstmCell({ units: rnn_output_neurons }));
    }

    model.add(tf.layers.rnn({
        cell: lstm_cells,
        inputShape: rnn_input_shape,
        returnSequences: false
    }));

    model.add(tf.layers.dense({ units: output_layer_neurons, inputShape: [output_layer_shape] }));
    console.log(model.summary()) */

/* model.compile({
    optimizer: tf.train.adam(learning_rate),
    loss: 'meanSquaredError'
});

await model.fit(xs, ys,
    {
        batchSize: batchSize, epochs: epochs, callbacks: {
            onEpochEnd: async (epoch, log) => {
                callback(epoch, log);
            }
        }
    }); */

// Split data into training and testing sets
const splitRatio = 0.8;
/* // @ts-ignore
const splitIndex = Math.floor(normalizedFeatures.length * splitRatio);
console.log('Split Index : ', splitIndex)
// @ts-ignore
const xTrain = normalizedFeatures.slice(0, splitIndex);
const yTrain = labels.slice(0, splitIndex);
// @ts-ignore
const xTest = normalizedFeatures.slice(splitIndex);
const yTest = labels.slice(splitIndex);

console.log('xTrain : ', xTrain.length, 'yTrain', yTrain.length)
console.log('xTest : ', xTest.length, 'yTest', yTest.length) */

// Create and compile the model
/* const model = tf.sequential();
model.add(tf.layers.dense({ units: input_layer_neurons, inputShape: [input_layer_shape] }));
model.add(tf.layers.reshape({ targetShape: rnn_input_shape }));

let lstm_cells = [];
for (let index = 0; index < n_layers; index++) {
    lstm_cells.push(tf.layers.lstmCell({ units: rnn_output_neurons }));
}

model.add(tf.layers.rnn({
    cell: lstm_cells,
    inputShape: rnn_input_shape,
    returnSequences: false
}));

model.add(tf.layers.dense({ units: output_layer_neurons, inputShape: [output_layer_shape] }));
model.compile({
    optimizer: tf.train.adam(learning_rate),
    loss: 'meanSquaredError',
}); */

// Define a custom callback to log loss after each epoch


// Train the model with the custom callback
/* model.fit(tf.tensor(xTrain), tf.tensor(yTrain), {
    epochs,
    batchSize,
    validationData: [tf.tensor(xTest), tf.tensor(yTest)],
    callbacks: {
        onEpochEnd: async (epoch, log) => {
            callback(epoch, log);
        },
    }
}).then(info => {
    console.log('Final loss', info.history.loss[epochs - 1]);

    // Test the model
    const testResult = model.evaluate(tf.tensor(xTest), tf.tensor(yTest));
    console.log('Test loss', testResult.dataSync());
}); */

/* model.add(tf.layers.reshape({ inputShape: [x_Train.length], targetShape: [x_Train[0].length, x_Train[0][0].length] }))
model.add(tf.layers.lstm({ units: 64, returnSequences: true }))
model.add(tf.layers.dropout({ rate: 0.2 }))

model.add(tf.layers.lstm({ units: 64, returnSequences: true }))
model.add(tf.layers.dropout({ rate: 0.2 }))

model.add(tf.layers.lstm({ units: 64, returnSequences: true }))
model.add(tf.layers.dropout({ rate: 0.2 }))

model.add(tf.layers.lstm({ units: 64 }))
model.add(tf.layers.dropout({ rate: 0.2 }))

model.add(tf.layers.dense({ units: y_Train[0].length })) */